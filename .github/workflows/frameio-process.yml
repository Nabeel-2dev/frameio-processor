name: Frame.io Folder Processor

on:
  repository_dispatch:
    types: [process-folder]

env:
  AIRTABLE_TOKEN: ${{ secrets.AIRTABLE_TOKEN }}
  AIRTABLE_BASE_ID: ${{ secrets.AIRTABLE_BASE_ID }}
  TABLE_NAME: "Result%20Folders"

jobs:
  process-folder:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours for large files
    
    steps:
    - name: Extract Parameters
      id: params
      run: |
        echo "folder_url=${{ github.event.client_payload.folderUrl }}" >> $GITHUB_OUTPUT
        echo "record_id=${{ github.event.client_payload.recordId }}" >> $GITHUB_OUTPUT
        echo "script_url=${{ github.event.client_payload.scriptUrl }}" >> $GITHUB_OUTPUT
        
        echo "üìù Parameters:"
        echo "  Folder: ${{ github.event.client_payload.folderUrl }}"
        echo "  Record: ${{ github.event.client_payload.recordId }}"
        echo "  Script: ${{ github.event.client_payload.scriptUrl }}"

    - name: Setup Processing Functions
      run: |
        # Function to call Google Apps Script with retry logic
        call_script() {
          local action=$1
          local params=$2
          local max_attempts=5
          local attempt=1
          
          while [ $attempt -le $max_attempts ]; do
            echo "üìû Calling: $action (attempt $attempt/$max_attempts)"
            
            local response=$(curl -s -L \
              --max-time 300 \
              --retry 3 \
              --retry-delay 2 \
              "$script_url?action=$action$params" 2>/dev/null)
            
            if [ $? -eq 0 ] && [ -n "$response" ]; then
              if echo "$response" | grep -q '"ok":true'; then
                echo "$response"
                return 0
              elif echo "$response" | grep -q '"ok":false'; then
                local error=$(echo "$response" | grep -o '"error":"[^"]*"' | sed 's/"error":"//;s/"//')
                
                # Check for retryable errors
                if [[ "$error" == *"quota"* ]] || [[ "$error" == *"bandwidth"* ]] || [[ "$error" == *"timeout"* ]]; then
                  echo "‚ö†Ô∏è  Retryable error: $error (attempt $attempt)"
                  sleep $((attempt * 3))
                  attempt=$((attempt + 1))
                  continue
                else
                  echo "‚ùå Non-retryable error: $error"
                  echo "$response"
                  return 1
                fi
              fi
            fi
            
            echo "üîÑ Request failed, retrying in $((attempt * 2))s..."
            sleep $((attempt * 2))
            attempt=$((attempt + 1))
          done
          
          echo "‚ùå Failed after $max_attempts attempts"
          return 1
        }
        
        # Export for use in subsequent steps
        echo 'call_script() {
          local action=$1
          local params=$2
          local max_attempts=5
          local attempt=1
          
          while [ $attempt -le $max_attempts ]; do
            local response=$(curl -s -L --max-time 300 --retry 3 --retry-delay 2 "'"$script_url"'?action=$action$params" 2>/dev/null)
            
            if [ $? -eq 0 ] && [ -n "$response" ]; then
              if echo "$response" | grep -q '"ok":true'; then
                echo "$response"
                return 0
              elif echo "$response" | grep -q '"ok":false'; then
                local error=$(echo "$response" | grep -o '"error":"[^"]*"' | sed '"'"'s/"error":"//;s/"//'"'"')
                
                if [[ "$error" == *"quota"* ]] || [[ "$error" == *"bandwidth"* ]] || [[ "$error" == *"timeout"* ]]; then
                  sleep $((attempt * 3))
                  attempt=$((attempt + 1))
                  continue
                else
                  echo "$response"
                  return 1
                fi
              fi
            fi
            
            sleep $((attempt * 2))
            attempt=$((attempt + 1))
          done
          
          return 1
        }' > /tmp/script_functions.sh
        
        # Set variables for subsequent steps
        echo "script_url=${{ steps.params.outputs.script_url }}" >> $GITHUB_ENV
        echo "record_id=${{ steps.params.outputs.record_id }}" >> $GITHUB_ENV

    - name: Initialize Folder Processing
      id: init
      run: |
        echo "üöÄ Starting Frame.io processing..."
        echo "Folder URL: ${{ steps.params.outputs.folder_url }}"
        echo "Record ID: ${{ steps.params.outputs.record_id }}"
        echo "Script URL: ${{ steps.params.outputs.script_url }}"
        
        source /tmp/script_functions.sh
        
        echo "üìÅ Step 1: Initializing folder..."
        init_response=$(call_script "initFolder" "&folderUrl=${{ steps.params.outputs.folder_url }}")
        
        if [ $? -ne 0 ]; then
          echo "‚ùå Init error: $(echo "$init_response" | grep -o '"error":"[^"]*"' | sed 's/"error":"//;s/"//')"
          exit 1
        fi
        
        echo "‚úÖ Initialization successful!"
        echo "$init_response" | jq '.' 2>/dev/null || echo "$init_response"
        
        # Extract group ID and asset information
        group_id=$(echo "$init_response" | grep -o '"groupId":"[^"]*"' | sed 's/"groupId":"//;s/"//')
        
        if [ -z "$group_id" ] || [ "$group_id" = "null" ]; then
          echo "‚ÑπÔ∏è  No files found for processing"
          echo "files_found=false" >> $GITHUB_OUTPUT
          exit 0
        fi
        
        echo "files_found=true" >> $GITHUB_OUTPUT
        echo "group_id=$group_id" >> $GITHUB_ENV
        echo "init_response<<EOF" >> $GITHUB_ENV
        echo "$init_response" >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV
        
        echo "üÜî Group ID: $group_id"

    - name: Ultra-Fast Parallel Upload
      id: parallel_upload
      if: steps.init.outputs.files_found == 'true'
      run: |
        echo "üöÄ Starting ultra-fast parallel asset processing..."
        
        source /tmp/script_functions.sh
        
        # Extract all asset information
        asset_data=$(echo "$init_response" | grep -o '"assetId":"[^"]*","fileId":"[^"]*","fileName":"[^"]*","fileSize":[0-9]*,"mimeType":"[^"]*","totalChunks":[0-9]*' | \
          sed 's/"assetId":"//g; s/","fileId":"/ /g; s/","fileName":"/ /g; s/","fileSize":/ /g; s/,"mimeType":"/ /g; s/","totalChunks":/ /g; s/"//g')
        
        if [ -z "$asset_data" ]; then
          echo "‚ùå No asset data found"
          exit 1
        fi
        
        echo "üìä Assets to process:"
        echo "$asset_data" | while IFS=' ' read -r asset_id file_id file_name file_size mime_type total_chunks; do
          echo "  üé¨ $file_name ($asset_id) - $total_chunks chunks - $((file_size / 1024 / 1024))MB"
        done
        
        # Ultra-aggressive upload function for single asset
        upload_asset_ultra_fast() {
          local asset_info=$1
          local asset_id=$(echo "$asset_info" | cut -d' ' -f1)
          local file_name=$(echo "$asset_info" | cut -d' ' -f3)
          local total_chunks=$(echo "$asset_info" | cut -d' ' -f6)
          
          echo "üöÄ [$asset_id] Starting ULTRA-FAST upload: $file_name ($total_chunks chunks)"
          
          local attempts=0
          local max_attempts=100
          local consecutive_failures=0
          
          while [ $attempts -lt $max_attempts ]; do
            attempts=$((attempts + 1))
            
            # Ultra-aggressive settings for maximum speed
            local response=$(call_script "uploadAsset" "&assetId=$asset_id&budget_ms=300000&window=16&pace_ms=25")
            
            if [ $? -ne 0 ]; then
              consecutive_failures=$((consecutive_failures + 1))
              if [ $consecutive_failures -ge 5 ]; then
                echo "‚ùå [$asset_id] Too many consecutive failures"
                return 1
              fi
              echo "‚ö†Ô∏è  [$asset_id] Call failed, retrying..."
              sleep 2
              continue
            fi
            
            consecutive_failures=0
            
            # Check for completion
            if echo "$response" | grep -q '"paused":false'; then
              echo "‚úÖ [$asset_id] ULTRA-FAST upload completed: $file_name"
              return 0
            fi
            
            # Check for errors
            if echo "$response" | grep -q '"ok":false'; then
              local error=$(echo "$response" | grep -o '"error":"[^"]*"' | sed 's/"error":"//;s/"//')
              echo "‚ùå [$asset_id] Error: $error"
              
              # Handle specific errors
              if [[ "$error" == *"bandwidth"* ]]; then
                echo "‚è≥ [$asset_id] Bandwidth throttled, backing off..."
                sleep 10
                continue
              elif [[ "$error" == *"quota"* ]]; then
                echo "‚è≥ [$asset_id] Quota hit, waiting..."
                sleep 5
                continue
              elif [[ "$error" == *"timeout"* ]]; then
                echo "‚è≥ [$asset_id] Timeout, retrying immediately..."
                continue
              else
                return 1
              fi
            fi
            
            # Extract and show progress
            local uploaded=$(echo "$response" | grep -o '"uploaded":[0-9]*' | sed 's/"uploaded"://')
            local total=$(echo "$response" | grep -o '"totalChunks":[0-9]*' | sed 's/"totalChunks"://')
            local percentage=$((uploaded * 100 / total))
            
            echo "‚ö° [$asset_id] BLAZING: $uploaded/$total chunks ($percentage%) - attempt $attempts"
            
            # Dynamic sleep based on progress
            if [ $percentage -gt 80 ]; then
              sleep 0.5  # Almost done, check frequently
            elif [ $percentage -gt 50 ]; then
              sleep 1    # Halfway, moderate checking
            else
              sleep 1.5  # Early stage, less frequent
            fi
          done
          
          echo "‚ùå [$asset_id] Failed after $max_attempts attempts"
          return 1
        }
        
        # Export function for parallel execution
        export -f upload_asset_ultra_fast
        export -f call_script
        export script_url
        
        # Launch all uploads in parallel with maximum concurrency
        echo ""
        echo "üöÄ LAUNCHING MAXIMUM PARALLEL UPLOADS..."
        pids=()
        asset_count=0
        
        echo "$asset_data" | while IFS=' ' read -r asset_info; do
          if [ -n "$asset_info" ]; then
            asset_count=$((asset_count + 1))
            
            # Start background process for this asset
            (upload_asset_ultra_fast "$asset_info") &
            pids+=($!)
            
            echo "üî• Launched parallel upload #$asset_count (PID: $!)"
            
            # Minimal stagger to prevent server overload
            sleep 0.2
          fi
        done
        
        # Get actual PID count from the background processes
        total_processes=$(jobs -r | wc -l)
        echo "üî• Total parallel processes launched: $total_processes"
        
        # Monitor all uploads with real-time progress
        echo ""
        echo "üìä MONITORING ULTRA-FAST PARALLEL UPLOADS..."
        
        completed=0
        failed=0
        start_time=$(date +%s)
        
        # Wait for all processes
        for job in $(jobs -p); do
          if wait $job; then
            completed=$((completed + 1))
            elapsed=$(($(date +%s) - start_time))
            echo "‚úÖ Process completed! ($completed processed, ${elapsed}s elapsed)"
          else
            failed=$((failed + 1))
            elapsed=$(($(date +%s) - start_time))
            echo "‚ùå Process failed! ($failed failed, ${elapsed}s elapsed)"
          fi
        done
        
        total_time=$(($(date +%s) - start_time))
        
        echo ""
        echo "üèÜ ULTRA-FAST UPLOAD RESULTS:"
        echo "  ‚úÖ Completed: $completed"
        echo "  ‚ùå Failed: $failed"
        echo "  ‚è±Ô∏è  Total time: ${total_time}s"
        echo "  üöÄ Average: $((total_time / (completed + failed)))s per file"
        
        if [ $failed -gt 0 ]; then
          echo "‚ùå Some uploads failed"
          exit 1
        fi
        
        echo "üéâ ALL ULTRA-FAST UPLOADS COMPLETED SUCCESSFULLY!"
        echo "files_processed=$completed" >> $GITHUB_OUTPUT

    - name: Finalize and Create Review Link
      id: finalize
      if: steps.init.outputs.files_found == 'true'
      run: |
        echo "üîó Step 3: Creating Frame.io review link..."
        
        source /tmp/script_functions.sh
        
        finalize_response=$(call_script "finalizeFolder" "&groupId=$group_id")
        
        if [ $? -ne 0 ]; then
          echo "‚ùå Finalize error: $(echo "$finalize_response" | grep -o '"error":"[^"]*"' | sed 's/"error":"//;s/"//')"
          exit 1
        fi
        
        echo "‚úÖ Finalization successful!"
        echo "$finalize_response" | jq '.' 2>/dev/null || echo "$finalize_response"
        
        # Extract review link
        review_link=$(echo "$finalize_response" | grep -o '"reviewLink":"[^"]*"' | sed 's/"reviewLink":"//;s/"//')
        
        if [ -z "$review_link" ]; then
          echo "‚ùå No review link found in response"
          exit 1
        fi
        
        echo "üé¨ Frame.io Review Link: $review_link"
        echo "review_link=$review_link" >> $GITHUB_OUTPUT

    - name: Update Airtable Record
      if: steps.init.outputs.files_found == 'true'
      run: |
        echo "üìù Updating Airtable record..."
        
        review_link="${{ steps.finalize.outputs.review_link }}"
        
        echo "‚úÖ Updating with Frame.io link: $review_link"
        
        curl -X PATCH \
          "https://api.airtable.com/v0/$AIRTABLE_BASE_ID/$TABLE_NAME" \
          -H "Authorization: Bearer $AIRTABLE_TOKEN" \
          -H "Content-Type: application/json" \
          -d '{
            "records": [
              {
                "id": "'"$record_id"'",
                "fields": {
                  "Low Quality Bit.ly Folder Link": "'"$review_link"'"
                }
              }
            ]
          }'
        
        echo ""
        echo "‚úÖ Airtable record updated successfully with Frame.io link"

    - name: Success Summary
      if: steps.init.outputs.files_found == 'true'
      run: |
        echo ""
        echo "üéâ ========== ULTRA-FAST PROCESSING COMPLETE! =========="
        echo "üìÅ Folder: ${{ steps.params.outputs.folder_url }}"
        echo "üÜî Record: ${{ steps.params.outputs.record_id }}"
        echo "üìä Files processed: ${{ steps.parallel_upload.outputs.files_processed }}"
        echo "üé¨ Frame.io link: ${{ steps.finalize.outputs.review_link }}"
        echo "‚ö° Processing method: ULTRA-FAST PARALLEL"
        echo "üöÄ Status: SUCCESS"
        echo "=================================================="

    - name: No Files Found
      if: steps.init.outputs.files_found == 'false'
      run: |
        echo "‚ÑπÔ∏è  No 'highlights' or 'teaser' files found in the folder"
        echo "‚úÖ Job completed successfully (no processing needed)"
